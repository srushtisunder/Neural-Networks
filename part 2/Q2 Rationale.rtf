{\rtf1\ansi\ansicpg1252\cocoartf1504
{\fonttbl\f0\fswiss\fcharset204 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;\csgray\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 The lower the batch size , higher will be the time taken by the network to train itself. This is because updating parameters is an expensive operation and if we chose a large batch size, we\'92ll get through the sample data faster. A good analogy is to imagine travelling a fixed distance being travelled but with differing stride lengths. We would maximise the stride length in order to cover the distance faster.We need to chose a batch size that is large but at the same time we want it to converge fast. Therefore we chose 32 for our mini-batch SGD\
}